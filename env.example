# Open CTF Environment Configuration
# Copy to .env and customize for your setup

# ==============================================================================
# PROVIDER CONFIGURATION
# ==============================================================================

# Use LiteLLM as the universal provider ((supports local ollama via 'ollama' provider keyword))
CYBER_AGENT_PROVIDER=ollama

# ==============================================================================
# LLM MODEL CONFIGURATION (Local Ollama)
# ==============================================================================

# LLM model for agent reasoning (Qwen3 recommended for CTF tasks)
CYBER_AGENT_LLM_MODEL=ollama/qwen3:8b

# Ollama server configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_API_BASE=http://localhost:11434

# ==============================================================================
# EMBEDDING MODEL CONFIGURATION (Local Ollama)
# ==============================================================================

# Embedding model for memory/RAG (mxbai-embed-large is fast and high quality)
CYBER_AGENT_EMBEDDING_MODEL=mxbai-embed-large
MEM0_EMBEDDING_MODEL=mxbai-embed-large

# ==============================================================================
# MEMORY SYSTEM
# ==============================================================================

# Use local FAISS for memory (no cloud dependency)
# Memory persists across sessions in ./outputs/<target>/memory/
MEM0_LLM_PROVIDER=ollama
MEM0_LLM_MODEL=qwen3:8b

# ==============================================================================
# OUTPUT CONFIGURATION
# ==============================================================================

CYBER_AGENT_OUTPUT_DIR=./outputs
CYBER_AGENT_ENABLE_UNIFIED_OUTPUT=true

# ==============================================================================
# OPTIONAL: CLOUD PROVIDERS (uncomment if using)
# ==============================================================================

# OpenAI
# OPENAI_API_KEY=your_key
# CYBER_AGENT_LLM_MODEL=openai/gpt-4o

# Anthropic (via LiteLLM)
# ANTHROPIC_API_KEY=your_key
# CYBER_AGENT_LLM_MODEL=anthropic/claude-sonnet-4-20250514

# Google Gemini
# GEMINI_API_KEY=your_key
# CYBER_AGENT_LLM_MODEL=gemini/gemini-2.5-flash

# AWS Bedrock
# AWS_ACCESS_KEY_ID=your_key
# AWS_SECRET_ACCESS_KEY=your_secret
# AWS_REGION=us-east-1
# CYBER_AGENT_PROVIDER=bedrock

# ==============================================================================
# OBSERVABILITY (Optional)
# ==============================================================================

# Langfuse for tracing (optional)
# ENABLE_OBSERVABILITY=true
# LANGFUSE_HOST=http://localhost:3000
# LANGFUSE_PUBLIC_KEY=your_public_key
# LANGFUSE_SECRET_KEY=your_secret_key
